#import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#import Data
df = pd.read_csv('kc_house_data.csv')
df.head(5).T

#get some information about our Data-Set
df.info()
df.describe().transpose()

# drop some unnecessary columns
df = df.drop(['date','id','zipcode'],axis=1)

X = df.drop('price',axis =1).values
y = df['price'].values
#splitting Train and Test 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

#standardization scaler - fit&transform on train, fit only on test
from sklearn.preprocessing import StandardScaler
s_scaler = StandardScaler()
X_train = s_scaler.fit_transform(X_train.astype(np.float))
X_test = s_scaler.transform(X_test.astype(np.float))

# Creating a Neural Network Model
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam

model = Sequential()
model.add(Dense(64,activation='relu'))
model.add(Dense(16,activation='relu'))
model.add(Dense(1)) #If you don't specify anything, no activation is applied (ie. "linear" activation: a(x) = x).
model.compile(optimizer='Adam',loss=tf.keras.losses.MeanAbsoluteError())

history = model.fit(x=X_train,y=y_train, validation_split=0.1,
          batch_size=128,epochs=100)
          
model.summary()

#loss_df = pd.DataFrame(model.history.history)
#loss_df.plot(figsize=(12,8))

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()

#compare actual output values with predicted values
y_pred = model.predict(X_test)

# evaluate the performance of the algorithm (MAE - MSE - RMSE)
from sklearn import metrics
print('MAE:', metrics.mean_absolute_error(y_test, y_pred))  
